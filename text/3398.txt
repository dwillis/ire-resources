Tipsheet #3398

New Frontiers in Computer‐Assisted reporting
IRE Las Vegas, June 2010
Sarah Cohen
Duke University
919.613.7348
sarah.cohen@duke.edu
Arguably, our fundamental reporting techniques have changed very little since the advent of e‐mail and the public
internet nearly 20 years ago. We still use databases, spreadsheets and mapping programs to work with structured
databases that we get through public records laws. While newsrooms are stuck, other fields have started mimicking
much of what we always wanted to do, and are starting to create tools and methods that over time we hope to
borrow. The key is to make them easy and free, something that the Knight Chairs and others are working hard to
bring to reporters.

Existing free tools
Many of the free new tools were designed to publish or for other purposes but can be hijacked for
reporting methods.
•

Outwit the Web: http://www.outwit.com/support/help/tutorials/ A simple Firefox add‐in that
automates some of the more tedious web‐extraction tasks, ranging from extracting all links from
a page to simple scraping of web sites.

•

Freebase GridWorks: http://code.google.com/p/freebase‐gridworks/ Throw away your Excel
spreadsheet when you have to standardize and filter large datasets.

•

Simile Exhibit and Timeline: A difficult‐to‐use, but pretty powerful, browser and timeline maker
for simple datasets. The old, but better organized, site is http://simile.mit.edu/wiki/Exhibit with
links to the current host at http://code.google.com/p/simile‐widgets/wiki/Exhibit

•

DocumentCloud: Go to other sessions on this, but it includes the great NY Times Doc Viewer for
annotated documents and simple “entity extraction” of names and places using OpenCalais.

•

NodeXL: A simple Excel 2007 plug‐in to work with networks. http://nodexl.codeplex.com/

•

Three visualization tools with different strengths:
http://manyeyes.alphaworks.ibm.com/manyeyes/, Tableau Public
http://www.tableausoftware.com/public and Swivel http://www.swivel.com/.

•

Public Comment Analysis Tool, http://pcat.qdap.net/default.aspx, University of Pittsburgh &
University of Massachusetts. The underlying software for this project was recently released open
source, but the comment toolkit can be used online by anyone interested in finding near‐
duplicate documents across large collections. Journalism‐related uses might include leglislation
across many states, or analyses of narrative fields in databases to find patterns that elude
simpler searching. It may also have potential to group stories, blog posts and government
documents together to distinguish unique stories, reducing time in backgrounding or beat
coverage.

•

Analyzethe.us: https://analyzethe.us/ , which combines network analysis with powerful text
analysis to combine documents. Getting data in is very difficult, but it shows some promise.

©2010 - Duplication prohibited without permission from Investigative Reporters and Editors, Inc.

•

Entity extraction engines are available through a variety of sources. These tools break text into
data by extracting names, places, dates, amounts and sometimes concepts from documents. Some
require plain text, others work with PDFs or HTML pages. They differ in their strengths. For
example, OpenCalais is easy to use and requires no software, but you cannot create custom
dictionaries to, say, clarify that St. Thomas is the name of a local congressman, not an island in
the Caribbean. All of these are freely available, but they are not easy‐to‐use commercial software
and have limited documentation or help.
o
o
o

o
o

o

OpenNLP: http://opennlp.sourceforge.net/
NLTK, http://www.nltk.org/, the Natural Language Tool Kit, one of the first open source
entity extractors. Its tutorial also gives you a good overview of Python.
GATE / Annie : http://gate.ac.uk/sale/tao/splitch6.html#chap:annie . The General
Architecture for Text Engineering began in the 1990s and has expanded to include a lot
of natural language tools. Annie (“A Nearly‐New Information Extraction System”) is its
entity extractor
LingPipe: Free for research or limited use. Its web page also lists most of the software
that competes with it: http://alias‐i.com/lingpipe/web/competition.html
LBJ: Part of the Learning Based Java toolkit incorporates the Illinois Named Entity
Tagger in a simpler interface http://l2r.cs.uiuc.edu/~cogcomp/asoftware.php?skey=LBJ
This tagger’s strength is that it less frequently incorrectly guesses that a word is a name
when it’s not – it has fewer false positives, which can get annoying in some of the others.
OpenCalais : http://www.opencalais.com/ , a project of Thomson Reuters, a semantic
extraction engine. This is the one used by many of the other tools, including
DocumentCloud.

Analysis of free text, audio, visual recordings and handwriting
Social scientists, digital humanities scholars and intelligence and security agencies are all leveraging new
methods to make sense of large collections of documents, web‐based and otherwise. Here are a few of
the projects that show some promise, once they are adapted to newsroom use:
•

Digging into Data project from George Mason University: “Using Zotero and TaPoR on the Old
Bailey Proceedings: Data Mining with Criminal Intent,” ongoing. This collection is made of about
120 million words, largely filled with stories of crime. The project is going to use text mining
tools to categorize cases and look at changes in the punishment of crime over time. The authors
say it will move history from the realm of the anecdotal to the realm of comprehensive review. It
is just getting underway, but could prove an interesting bridge to reporting on crime and courts.

•

CERATOPS: Homeland security project geared at automatically extracting opinion and
summarizing events from free text: http://www.cs.pitt.edu/mpqa/ceratops/

•

The Muninn Project, http://www.muninn‐project.org/, which is automatically extracting
information from millions of hand‐written forms created during World War I, and moving them
into a database. The potential for extracting data from handwritten forms could be a huge
breakthrough for reporting.

•

Sphynx, a DARPA‐funded speech recognition project from Carnegie Melon:
http://www.speech.cs.cmu.edu/ This underlying technology is well suited for use on audio and
video hearings, legislative committee meetings, county council videos and court proceedings
unavailable as transcripts, along with recorded interviews. It is particularly geared more difficult
transcription that has many actors and voices, unlike some of the commercial products.

©2010 - Duplication prohibited without permission from Investigative Reporters and Editors, Inc.

•

SPINE, http://www‐speech.sri.com/projects/spine/ , another DARPA funded research project
that stands for “Speech in Noisy Environments.” I am seeking more information on this.

•

Political methodology breakthroughs. Examples include Gary King’s work at Harvard, especially
in categorizing large quantities of unstructured text: http://gking.harvard.edu/files/abs/discov‐
abs.shtml; Daniel Hopkins at Georgetown University with King on Readme: Software for
Automated Content Analysis, http://gking.harvard.edu/readme/

•

Jigsaw, a project of Georgia Tech under contract to Homeland Security, called “visual analytics for
Investigative Analysis.” The lead researcher, John Stasko, is interested in working with reporters
and expand the project beyond homeland security.

•

Center for Intelligent Information Retrieval at the University of Massachusetts, experiments with
George Washington’s writings: http://ciir.cs.umass.edu/irdemo/hw‐demo/

•

University of Maryland’s Center for Advanced Study of Language has a project called “Detecting
deception in interviews,” geared at finding lies when interviewing terrorism suspects. Its findings may
be applicable to journalism someday. http://www.casl.umd.edu/node/533

©2010 - Duplication prohibited without permission from Investigative Reporters and Editors, Inc.

