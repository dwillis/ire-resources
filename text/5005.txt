Editors: Bulletproofing Your Data Stories

By Joel Engelhart, investigations editor, The Palm Beach Post
(561) 820-4732; jengelhardt@pbpost.com; @JoelEpbpost
and Jennifer Forsyth, deputy investigations chief, The Wall Street Journal
(212) 416-3741; jennifer.forsyth@wsj.com; @ForsythJenn

--Even if you don’t know how to program or do data analysis yourself, asking the right
questions can go a long way toward identifying problems before publication or broadcast.
Here are some questions editors can ask:
•

How can I be sure that you didn’t double count fields? (The reporter should be able
to show you how he or she checked for this.)

•

Do we have the universe or just a subset? If just a subset, how did we choose it and
how to we know it is representative of the whole?

•

You have a percentage or total in the story, but how many people (or companies,
etc.) is that calculation based on? (66% looks less impressive if you realize the
reporter only talked to 3 companies.)

•

If you used data from a source other than the original, how do you know if it’s up-todate? Can we check it with the original source? (For example, Open Secrets data can
be behind FEC data.)

•

Was the data self-reported? If so, was rigorous vetting of the data enforced? How do
you know?

•

You matched names, but didn’t have birthdates or social security numbers. How did
you verify the matches? And did you do that verification every single time or just
spot check?

--With data analysis, it’s particularly important for reporters to run their methodology by
experts. Even the most experienced data reporters do so. It’s much better to find out about
flaws than to have people pick it apart after publication or broadcast. Editors can ask these
additional questions:
•

Did you have to do any data entry yourself and, if so, who checked it behind you?

•

Have you talked about the data with the agency that produced it? Do you
understand how it was collected, and for how long, and what its purpose was?

•

Did you run your data analysis by an expert in the field? How did you find and vet
the expert? Was the expert already familiar with this data?

•

Did you ask the expert: “What am I missing? What could I have done wrong?”

•

Have you thoroughly explained your data analysis to the subject(s) of your
investigation? Did they push back? If so, how?

--Sometimes there isn’t data that’s readily available and reporters build their own from
surveys/interviews/documents. Plan on spending a considerable amount of time strategizing
with the reporter on how best to do this and whether the effort is worth the time involved to
do it right.
Here are some common bugaboos in collecting your own data:
•

Sample isn’t big enough. (How many cities, colleges, school districts, etc. is enough?)

•

Data sample isn’t the entire universe or representative of the universe.

•

Time period isn’t long enough. We don’t have enough data to look at meaningful
change over time.

•

Data we’ve gathered is cherry picked and is influencing the outcome.

•

The data was self-reported or has too many holes.

Here are some good practices that can give you another level of comfort.
•

Sit by the reporter and make him or her show you the spreadsheets and
calculations. Even if you aren’t a programmer, you’ll be surprised at the questions
you find yourself asking.

•

Ask reporters to send in a detailed explanation of their methodology early in the
process. Edit it at that point as if it were being published immediately, both for
clarity and for logic. If you don’t understand it, the story itself isn’t ready for editing.

•

Ask the reporter for the record layout so you know what fields she chose and what
the whole data set had in it.

•

Resist the urge to “write around” what is inconvenient in the findings or what is not
easily explained. Point out problems with the data and your unknowns—or your
readers and critics will.

•

Don’t publish complex data stories on deadline. Don’t.

