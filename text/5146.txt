How to reach me:
Twitter: @maggiekb1
Maggie.koerth-baker@fivethirtyeight.com
You should check the notes from the Thursday “Research Oasis” Session, because
it had several useful research resources.
Also, for more information on the “technically illegal paywall cracking” sites that
Melissa mentioned, see:
Examples of my stories where putting single papers into the broader context of a
field of research was vitally important:
https://fivethirtyeight.com/features/why-we-dont-know-how-much-sexism-ishurting-clintons-campaign/
- Individual studies suggest sexism no longer affects the ability of women to
run for office and win. BUT America still lags behind the rest of the developed
world in female candidates and female politicians. So what gives? Deeper dive
into the context shows that the disconnect is coming from factors that are
difficult to study quantitatively.
https://fivethirtyeight.com/features/all-those-new-dinosaurs-may-not-be-newor-dinosaurs/
- There’s a new dinosaur species discovered every week, on average. Super
cool! But deeper investigation of the research context shows that 48 percent of
named dinosaur species later turn out to be rejected – for lack of data, because
they represent species that were already known, or because they weren’t
dinosaurs at all.
How to find the broader context:
3 Kinds of studies that compile multiple studies into one place:
Reviews are: sort of like a book report or really more like Cliff’s notes.
Researchers go gather a bunch of different articles on the same topic and kind of
summarize what is known, what isn’t known, and what needs more research.
Meta-analyses are: linking a bunch of studies to make one big study. Multiple
studies of the same thing done in close enough to the same way that you can
statistically combine their data

Systematic reviews are: more like a study of studies. Truth is that you are very
lucky if you’re ever able to neatly combine data like a meta-analysis does – unless
they were designed with that in mind from the start. Pharmaceutical companies
do meta-analyses frequently when they produce data for drug approval. But
systematic reviews kind of allow you to work with reality as it exists. Scientific
exploration of data that can’t be easily combined. Based around answering a
question or hypothesis. Pre-determined criteria for what papers will be included
and what won’t. Use of statistical techniques, at the very least for scoring the
quality of evidence. Usually peer reviewed.
In practice: Most of what I look at is systematic reviews. Cochrane Collaboration
being one of the big, best known ones. Great place to look for evidentiary context
on medical treatments and therapies. http://www.cochrane.org/
Generally these are more reliable than single studies. BUT: You won’t find them
for everything. The places where you find lots have some problematic incentives
at work. So, for instance, it’s often hard to find good systematic reviews of social
science research; it’s often easy to find a dozen systematic reviews of a single
pharmaceutical or medical question. Good reviews take a lot of time. Not every
field rewards them. Medicine has begun to reward them – but also now started to
be used as marketing tools to produce a specific answer on behalf of a company.
Tread carefully. Look at who is doing the review/meta-analysis/systematic review
and what ties they have to the people it serves.
http://retractionwatch.com/2016/09/13/we-have-an-epidemic-of-deeply-flawedmeta-analyses-says-john-ioannidis/
Other limitations:
- not everything gets published – small studies with “interesting” results are
more likely to get published than small studies that say “nope, nothing here”. So
what you’re looking at in a systematic review, review, or meta-analysis is almost
always going to be biased. It’s probably missing the “and then nothing happened”
results.
- likewise, they tend to be biased to English-language research. And that can
actually be really significant, because cultural and genetic factors can differ a lot
from one part of the world to another and those can affect outcomes. If you’re
not getting the results from China, Japan, Russia, etc, there’s a lot you probably
don’t know.

- You don’t know how good a librarian the authors had, or how honest they
were. Does what’s here represent a full range of literature, or has it been selected
through laziness, lack of time/funding/skill, intention? You honestly have very
little clue. Remember that there are literally millions of scientific papers published
every year. Nothing includes everything.
- It’s a mistake to think that the results of the average apply to an individual. A
systematic review or meta-analysis might show that a drug works and is safe, and
it can still cause deadly side effects and little benefit in a single, particular patient.
The average =/= everybody. Variability matters.
So what do you do?
1. Triangulate through interviews
- what are the weaknesses of your paper? What questions could you not answer
that you wish you could?
- what are the big controversies in your field?
- who is a researcher that disagrees with you that you’d recommend I speak with?
- who is writing the textbooks on this subject? Go talk to them.
- response papers (look at citations in google scholar, any published responses to
the paper you’re writing about will show up there)
- obviously biased sources and their usefulness on background. Yup, they are
employed by a think tank to oppose this research. That doesn’t mean they can’t
be useful in figuring out what you don’t know. What do they think the
weaknesses of the paper are? Sometimes, they’ll hit on something interesting.
Sometimes not. But if you’re having trouble getting any kind of info on
weaknesses, they might be a good place to turn.
2. Understand that science is an industry, shaped by professional and financial
incentives and that affects the body of knowledge.
- universities are not unbiased sources. But not necessarily in the way you’re
imagining. Doesn’t mean they’re politically biased. But they are ‘we’re awesome’
biased. Being seen as awesome is currency.
- science is biased to “woah, that’s weird”. Especially the top journals like science
and nature.
- scientists are incentivized to publish A LOT. Their jobs depend on it. Especially
when they are young. That incentivizes them to publish really piddly little things
that might not matter much to anybody. It also incentivizes cutting corners.

- science is dependent on replication. Science as an industry does not incentivize
these things. Unless results really throw up red flags or the topic is controversial,
it’s pretty likely nobody has tried to replicate it.
- p-hacking. P-value or calculated probability that the answer is correct. .05 has
long been the ticket to publication in psychology, biosciences, social sciences. And
it’s 100% game-able. it’s easier to get a result than an answer. Christie’s story,
interactive that will allow you to see how you can make either Republicans or
Democrats better for the economy depending on the criteria you use and
variables you focus on. https://fivethirtyeight.com/features/science-isnt-broken/
- if you’re going to write about science: read the Ionnadas paper from 2005
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1182327/
http://www.cureffi.org/2016/03/17/john-ioannidis-the-state-of-research-onresearch/
http://www.stat.cmu.edu/~ryantibs/journalclub/ioannidis.pdf
3. Work hard on conveying uncertainty to your readers
- one of the ways Christie framed this that I really like “every result is a
temporary truth”
- the flaw in how we teach science – absolute facts that must be memorized
– you have the opportunity to correct that in your writing.
- don’t be too in awe of a single technique. Don’t say: “not THIS PROVES”
but rather, “this is how scientists are trying to demonstrate” or “this suggests a
possible connection”
- remember everything has caveats
- how likely is this to be true? What questions could lead us to change our
understanding?
- the best way of understanding this: with climate change, we’re pretty unlikely to just
suddenly discover that extra carbon dioxide in the atmosphere doesn’t lead to planetary
warming trends or that that those trends don’t have all kinds of weird effects. But we’re likely
to learn a lot more about the dose/response relationship, about what kinds of weird impacts
are more or less likely, about what the best ways to deal with it are.

