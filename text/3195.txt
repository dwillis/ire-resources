Computational Journalism and the 5 Ws

NICAR 2009 Conference

March 21, 2009

James T. Hamilton, Director, DeWitt Wallace Center for Media and Democracy, Duke jayth@duke.edu
WHAT is Computational Journalism?
I view computational journalism as the combination of data, algorithms, and knowledge from
social science to yield information that can supplement and, in the future, substitute for part of
journalism’s watchdog function. By supplement, I mean that analyses like text mining and cluster
analyses can generate electronic tips that lower the costs to reporters of deciding what and where to
investigate. By substitute, I mean that eventually some watchdog articles will be written by algorithm, in
a way that would allow readers to see a customized, personalized article about how a policy problem is
playing out in their neighborhood, block, or lives.
For more on how to define CJ, see Nieman Reports Spring 2009, “Tracking Toxics When the Data are
Polluted: How Computational Journalism Can Uncover What Polluters Would Prefer to Hide,” by James
T. Hamilton; “Deep Throat Meets Data Mining” by John Mecklin at http://www.miller‐
mccune.com/article/deep‐throat‐meets‐data‐mining/4 ; and the proceedings of the first CJ conference,
organized by Irfan Essa and Nick Diakopoulos at http://www.computational‐
journalism.com/symposium/index.php .
WHY is it needed?
As economist Anthony Downs has pointed out, people have four different types of information
demands: producer, consumer, entertainment, and voter. The markets for the first three types of
information work fairly well, since if you don’t seek out the data you don’t get the benefit. If you want a
better car, you do your research, and you get a better car or a lower price. But for voter information,
since your vote does not materially affect the outcome of an election, most people choose to be
“rationally ignorant” about the details of politics. This means there is a gap between what people need
to know as citizens and what they actually want to know as readers/viewers. For many years owners or
managers were willing to subsidize public affairs coverage, in an era when you could do well and do
good since you were earning high profit rates. Today, those margins and that willingness to provide
accountability/investigative coverage are dwindling fast. Watchdog/accountability coverage at the local
level may be the type of news most at risk of disappearing.
CJ offers a way to reduce the costs of discovering and producing investigative reports, and a way
to increase demand by personalizing the impacts of public policy. CJ can lower the costs of detecting
corruption and inefficiencies. CJ also holds the promise of creating narratives that can be produced by
algorithm and consumed by readers who want to know how their particular lives are affected by
particular policies.
For more on news economics, see All the News That’s Fit to Sell: How the Market Transforms
Information Into News, James T. Hamilton (Princeton Press, 2004) and 2008 Breaux Symposium: New
Models for News at http://www.lsu.edu/reillycenter/Breaux_NewModels‐News_Web.pdf

©2009 - No duplication without permission from Investigative Reporters and Editors, Inc.

Where can you find examples of CJ?
Research in computational journalism will eventually generate tools that take government data
feeds, media coverage, and user‐generated content to develop indicators of social outcomes,
descriptions of government operations, and narratives that attract public attention and possible
solutions to collective problems. Creating the technologies and algorithms to “report” on local
communities will focus attention on data volume, heterogeneity, quality, and provenance. This will
involve interaction among journalists, computer scientists, social scientists, policy analysts, and
government data providers.
How might CJ work with today’s technology? In the Spring 2009 Nieman Reports I offer the
following examples of how a reporter could investigate local chemical data provided by the Toxics
Release Inventory data, and use these examples to discuss what CJ could look like.
Statistical Analysis: Some numbers follow Benford’s Law, which means that if you look at the
distribution of first digits that 1s outnumber 2s, which outnumber 3s, and so on. Work in forensic
accounting shows that when people fudge their numbers they forget to do it in a way that replicates
Benford’s Law. This means that analysis of first digits is a way to check the accuracy of self‐reported
data. In looking at the actual level of pollution near plants reporting TRI emissions, Scott de Marchi and I
found that for the chemicals such as lead and nitric acid the measured pollution concentrations around
plants followed Benford’s Law. But the self‐reported data did not. This suggests that for these two
heavily regulated chemicals, TRI reports may not be accurate. The TRI form actually provides a name and
phone number if the public has questions about a plant’s reports, which could be a starting point for
reporters investigating the accuracy of TRI data.
No change: If facilities are not serious about their pollution estimates, they may fall back on a simple
rule of thumb – simply report the same pollution figures each year. I got the idea for this test after
reading David Barstow and Lowell Bergman’s Pulitzer prize‐winning investigation of workplace injuries
and deaths at McWane manufacturing plants. Thinking that a company willing to violate workplace
regulations so egregiously would not be likely to invest much time in estimating its pollution releases, I
looked and found that TRI reports at McWane at times simply remained the same from one year to the
next. When Scott de Marchi and I checked nation‐wide we found that plants where pollution levels
stayed the same across years were likely underreporting their actual emissions. Journalists looking for
underreporting can start by seeing which local polluters report the same figures year to year.
Visualization: I have found across environmental programs that even after you take into account
income and education levels, areas with higher voter turnout get better levels of environmental
protection. The higher the voter turnout in an area, the greater the reduction in air carcinogens, the
more stringent the Superfund cleanups at hazardous waste sites, and the lower the chance that
hazardous waste processing capacity will be expanded in the area. One way for online news sites to
show the relation between pollution and politics is to show how voting rates differ around polluting
facilities. In a state such as North Carolina, you can purchase from the State Board of Elections the voter
registration file for the entire state for $25. This shows the addresses of registered voters, and whether

©2009 - No duplication without permission from Investigative Reporters and Editors, Inc.

they turn out at the polls too. Voter address data can be used to create maps that show how political
activity varies across streets and neighborhoods.
Mapping: The TRI forms provide the address for where polluting facilities ship their toxics for offsite
disposal. The records generated by the Resource Conservation and Recovery Act (RCRA) contain similar
information for where nonmanufacturing firms send their waste. In my environmental policy classes, we
track where (radioactive) medical waste from Duke ends up across the country. These data would allow
reporters to show whose toxics are ending up in your local area, and what neighborhoods in the US end
up receiving waste shipped from your area.
Matching: To find people who should have reported TRI emissions but did not, regulators initially
compared local business directories and manufacturing lists with the facility name and addresses of TRI
filers. In some industries, the production process almost by definition would entail the release of
particular chemicals. The easy availability today of detailed information on plants and facilities makes
this a matching process that journalists can conduct on their own. This would be especially helpful in
discovering violators when programs are new and word of requirements has not traveled widely.
Personalization: Right now the EPA collects pollution data from multiple programs into the EnviroFacts
database, and merges environmental data with community information in the EnviroMapper function.
In the future algorithms could take this information and write the story of your local environment,
letting you know likely exposures from different facilities, types of enforcement actions recently taken at
nearby plants, trends over time in local public health, pictures of emitting facilities, and how these
pollution patterns compare across time and across other areas in your city.
Data from the TRI can at times be controversial, imperfect, and (strategically) slanted. Yet as the
field of computational journalism develops, the use of algorithms and knowledge from social science
should allow reporters to use data such as the TRI to lower the cost of generating watchdog coverage
and raise interest in political issues by personalizing the impacts of public policies. For more on the TRI,
see Regulation through Revelation: The Origin, Politics, and Impacts of the Toxics Release Inventory
Program, James T. Hamilton (Cambridge University Press, 2005).
Who will be doing CJ?
CJ builds on CAR and Precision Journalism, using larger data sets and better algorithms. The
award‐winning CAR work of today shows where the field can be headed. Sites like Everyblock, Sunlight
Labs, Many Eyes, Swivel, and papers such as the New York Times, Washington Post, and Seattle Times
contain the building blocks of CJ. Ideally academia can spur the development of open‐source CJ tools.
When will this take place?
Upcoming events in evolution of CJ: Summer 2009 Duke hires first Knight Chair in computational
journalism; Fall 2009 release of report from the workshop Developing the Field of Computational
Journalism, to be held in summer 2009 at the Center for Advanced Study in the Behavioral Sciences at
Stanford; beyond… to be defined in part by the journalists in this room.

©2009 - No duplication without permission from Investigative Reporters and Editors, Inc.

