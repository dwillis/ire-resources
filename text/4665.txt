Advanced data cleaning with Python:
Machine learning techniques
Hands-on session with dedupe and usaddress
NICAR 3/10/16
Forest Gregg
fgregg@datamade.us
Cathy Deng, @cthydng
cathy.deng@datamade.us

What problem are we solving?
Data is messy/ambiguous:
- When it is entered by humans
- When it involves free text
- When there aren’t unique identifiers
TL;DR data is messy/ambiguous all the time

Why is this a challenging problem?
Language is quirky & flexible
John Smith
john s
Jonathan Smith
Mr. J Smith
John A. Smith

Smith, J A
j smith
John A Smiht
Mr John S.
….etc

Why is this a challenging problem?

The treachery of
images data

A name is not a person

Example: campaign finance data

Why is machine learning a useful
approach?
Why machines?

Why learning?

Machines can work at scale
Writing rules to clean data can
Machines can leave an audit trail get real hairy real fast

The challenges:
How can computers know if names are similar?
How can computers know if similar addresses matter more
or less than similar names or similar employers?
How can computers cluster similar records quickly if there’s
a lot of data?

A data cleaning workflow
What a human does

teach dedupe about what similarity means in a
particular dataset, by judging a few examples
What a machine does, that humans can’t

apply “lessons” about similarity to new records,
at scale

Hands-on session outline
1. dedupe CSV example
a.
b.
c.
d.
e.

input is dictionary
missing data is None
data model
user interface
output

2. dedupe for bigger data
3. usaddress

Tools for data cleaning (all open source!)
de-duplication & entity resolution
dedupe: https://github.com/datamade/dedupe

structuring the unstructured
usaddress: https://github.com/datamade/usaddress
probablepeople: https://github.com/datamade/probablepeople
parserator: https://github.com/datamade/parserator

Questions?
Contact us!
Forest Gregg
fgregg@datamade.us
Cathy Deng, @cthydng
cathy.deng@datamade.us

