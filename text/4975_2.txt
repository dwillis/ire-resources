Algorithmic fairness

Sharad Goel
Stanford University

Are algorithms like
COMPAS fair?
It’s complicated.
[ Disparate impact is not discrimination ]

From risk scores to decisions
Step 1: Estimate defendant’s likelihood of committing violent
offense based on available info, such as criminal history.
Step 2: Set a detention threshold (e.g., detain defendants
deemed more than 20% likely to commit a violent offense).

Black defendants are detained
more often than whites
Algorithm does not use race.

Why are blacks detained
more often than whites?
•

Defendants with more serious criminal
histories are more likely to reoffend and
thus more likely to be detained.

•

Black defendants on average have more
serious criminal histories.

Error rates are also higher
for blacks than whites
Similar phenomenon.
[ see “Algorithmic decision making and the cost of fairness” ]

Are the data biased?
Reported crime is a proxy for actual crime.
But serious crime is ostensibly less susceptible to
bias than minor crime.
Unclear how big the effect might be, or even whether
the data are biased against whites or blacks.

What can [ should ] we do?
Option 1: Stop using algorithms
Judges are trying to do the same thing as algorithms
(determine who’s risky), but are typically worse at it.

What can [ should ] we do?
Option 2: Set race-based detention thresholds
Detain whites who are at least 20% likely to commit a
violent crime, and blacks who are at least 30% likely.
Probably violates the Equal Protection Clause.

What can [ should ] we do?
Option 3: Move away from pre-trial detention
Only detain defendants who pose serious threat to
public safety. Bolster pre-trial services.
[ Use algorithms to help determine who is risky ]

