Avoiding the big error: What to watch out for in data
IRE TIPSHEET

FORT WORTH 2006

John Maines
Assistant News Editor/CAR
South Florida Sun Sentinel (Fort Lauderdale)
954-356-4737
jmaines@sun-sentinel.com
Avoiding the big error is a combination of proper use of data tools and common sense.
Sometimes, the user of a database manager or spreadsheet will be so closely watching
that that the math is correct that they’ll forget to check if the final results make any sense.
And when we send those results to the editor or graphics desk, it’s usually the first thing
they spot (gulp).
Databases:
Keep a clean copy of the original. A version that is absolutely untouched. If
things go wrong, you can back up and start over with a clean version.
Check the records in the database against a totals provided by a source.
Do they match? Does the database contain all the records? (Example: In
Florida, the state sells a database of concealed weapons permits. But unless
you ask, the state only provides only existing permits, not those that have been
suspended or revoked.)
Read the fine print. Data standards change over time. (Example: On the U.S.
Census Bureau’s American Factfinder web site, the 2004 total population
numbers do not include people living in nursing homes, dormitories, military
bases, etc. The 2000 Census includes those categories. Things are not always
equal, even from the same source.
Check for nulls. Just because the database has a unique identifier field
doesn’t mean it’s always filled in. Other important fields might also have many
nulls. Why? Ask the data provider.
Check for the quality of the data. A name might be entered many different
ways. CitiCorp might show up as CitiBank, CitiGroup, CityCorp Inc., etc. Same
thing with individual names – Bob Smith, Robert Smith, Robert C. Smith.This
raises hell with analysis, and could require endless hours of data cleaning.
(Time to call the editor – this project’s going to take longer than we thought!)

Look for outliers. Numbers that don’t make sense or are artificially low.
(Example: Home sales that are listed for $100 because of a quit claim deed).
These should be deleted before work begins. If you’re unfamiliar with handling
outliers, contact the agency that provided the data.
Maps:
White areas. Take a close look at areas where no data is being recorded.
Sometimes there answer makes sense (for example, no housing sales
indicated in the census tract where the airport is located). Other times,
something went wrong in the mapping process.
Watch for geocoding problems. Geocoding is the process is where a
latitude/longitude is assigned to a street address, for mapping purposes. Your
mapping software does this. But sometimes the software can’t find the street
address. (Example: The mapping software lists an address as1357 U.S.
Highway 1, but your data source has it as 1357 Federal Highway). Pay close
attention to the addresses that the geocoding processes fails to locate. You
might need to do some data cleaning.
Does the color-coding make sense? It’s easy to make mistakes when
adjusting the settings on mapping software. Double-check the settings, and
watch for problems. Does the map look right, based on what you know?
Graphics:
Does the graphic match the numbers in the story? This mistake is much
easier than it sounds. Perhaps your analysis has changed slightly since the
graphics assignment was turned into the art department. Maybe the artist made
a mistake. Catch this before the copy desk does.
Rounding problems. A constant headache. When large numbers are rounded,
the sum of the parts is not always equal to the total. Here’s where you need
that disclaimer in a graphic about rounding.
Check your work:
Find experts. Do your finding match other people’s findings? If not, why not?
Might be a mistake. Or, just maybe, you found a great story. (But bet on the
mistake).
Double-check your work. Get the same results each time?
Find more tipsheets on data work at http://www.ire.org/resourcecenter/
--30--

