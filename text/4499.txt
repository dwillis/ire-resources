From Facts to Data:
Making Your Stories Airtight
IRE 2015
Tips and techniques to improve accuracy, reduce legal risk and keep your credibility intact as
you tackle complex investigative stories. The session includes tips for making meaning out of
data while keeping your facts straight and staying safe from all-too-common data journalism
pitfalls.
David Donald
Data Journalist in Residence Investigative Reporting Workshop & School of
Communication, American University @david_donald; donald@american.edu
Alleen Brown
Assistant Research Editor, The Intercept alleen.brown@theintercept.com
Shawn McIntosh
Deputy Managing Editor, Investigative & Data Journalism, The Atlanta JournalConstitution @shawnmcintosh; smcintosh@ajc.com

Fact-checking tips for reporters and editors
Reporting is often quick analysis of information. Returning to info after the piece has been
written inevitably surfaces errors. Writers should have a fact-checking process no matter how
meticulous the reporting, and editors should be questioning reporters’ sourcing.
•
•
•
•
•
•

•

Fact-check as you go – record where exactly you sourced each note.
Clearly note any information shared “off-record.”
Record all your interviews, including phone interviews (Recommended method: digital
recorder + headphones with mic + headphone splitter + audio cable.)
Tell your sources you’re recording, and know the laws in your state and your source’s
state around recording and consent.
To avoid being led astray by a lying or inaccurate source, find more sources.
Memory can be tricky and change with time, even about something a source directly
experienced. Different recollections don’t necessarily indicate anyone is lying, but
corroborate as much as possible.
Understand the source’s motivations, your own motivations.

•
•

•
•
•
•
•
•
•
•
•

Talk to everyone who was there, especially those you criticize.
Go beyond a request for comment – make clear what you’re saying about the subjects of
your story and allow them an opportunity to understand and respond. Try different
methods of reaching them and give enough time.
Read the complaint, but also read the opposing legal team’s response.
Read the case docket.
Seek the documents that informed the lawyers’ arguments (not just the malpractice suit,
also the medical records).
With stats and surveys, make sure there aren’t updated or more comprehensive studies
out there.
When you write, don’t just rely on notes – actually re-read your backup material.
Once you have a working draft, do a line-by-line review of the evidence that supports
your piece.
Ask yourself: In what way could this turn out to be wrong? What would the consequences
be if it were?
Avoid allowing sunken time or false deadlines to force you to publish a half-baked story.
A killed or delayed story isn’t failure.
It’s often the peripheral characters (in text or photo) who are most upset about how
they’re represented (i.e. the people who aren’t necessarily the focus of the most serious
checking, but who may make life the hardest after the fact)

Airtight data-driven stories
For the data journalist:
•

•

•

•

Start with creating a data diary or data log as soon as you begin the project. You’ll record
everything in it. It will seem time consuming but save you lots of time – and headaches –
as your analysis is edited and potentially challenged. This can be done on paper, or you
can use a note-taking app, such as Evernote or OneNote.
Make sure you ask when requesting the data (or FOIAing the data) for the accompanying
data dictionaries, record layouts and code sheets. They will be invaluable when important
and working with the data in eliminating guesswork.
Talk to the primary data sources. These are the people who create and work with the
data as a primary part of their jobs. They’re the database admins, data entry clerks, etc.
who are responsible for the data. Also, examine any forms that underlie the data, such as
the paper filled out when the data are collected or the instrument used if a survey.
Talk to the secondary data sources. These are the people who have used the data, often
for research or some other analytical task. They might be government economists or
accountants. They might be academics. It just depends. But the key is to understand

•

•

•

•

•

•

what they know to be the pitfalls in the data. Further, they already likely have a
methodology for analysis that you can steal.
Perform integrity checks. Do you have the number of records you were promised? Do the
electronic records match the hard copies if you entered them by hand? How clean are the
data in each column (field, variable, etc.)? Don’t just look for misspellings. What strange
codes did you find that aren’t on the code sheet? How much missing data do you have?
In one sense, this is data cleaning. But you have to go beyond simple data cleaning to
check both the original data and the effectiveness of your own data cleaning.
Make sure you spend time with the data. I call this “hanging out with the data.” It simply
means entering enough the data into your subconscious or unconscious part of the brain
and let those work on it. If you shortchange this step, you’ll likely find less than what you
hoped for when you decided to go for the story.
Keep asking, “What am I missing?” When you see big spikes or drops over time in your
data, that suggests an underlying reason that you may not be thinking about. Why did
test scores drop 8 points? It’s not that kids necessarily got dumber. It might simply mean
the test was re-normed, and such a drop was expected and within the typical standards
for re-normed tests.
Find eyes to look at your results. These might be on the academics that gave you the
original insight into the data. Now that you’re finding results, do they seem within the
expectations of the expert? Do the primary sources know something that could poke
holes in your findings? So when you first work with those sources on the front end,
indicate you may come back to them for more help on the backend.
Fact check everything going into the story. Do the analysis from start to finish at least
twice. If you work on a data team, have another member go over your analysis (including
your math and stats). If you’re the only data. My rule when working without a data check
from team member was to do the analysis from start to finish three times with the same
result each time. If one of the three was different, I’d start my “rule of three” over again.
Work with your editor. They likely don’t have the data training and skill that you have.
You’ll know how to word the sentences with your results, round the numbers that need
rounding, and use the appropriate weasel words that keep the data analysis real.

Bullet-proofing is really a methodology. If you take control of your data process, the
methodology section of your project almost will write itself.
For the editor or producer working with the data journalist:
Here’s a checklist we used at the Center for Public Integrity so that editors not trained in data
journalism could ask the right questions.
Editor’s checklist for data-driven stories

□ Has the staff reporter explained to the data team how the database was acquired or how the
database was created?
□ Has the staff reporter talked with anyone who has helped create or used the data and
communicated what he or she learned to a data team member?
□ Did a data team member check for duplicate records?
□ Have the names and other entities been cleaned?
□ Did a data team member check for aggregate numbers, examine how aggregation was done
and ensured there is no double counting?
□ Did a data team member look for outliers?
□ Have all numbers and statistics in the story been verified by someone other than the data
analyst?
□ Are the numbers based on counting of actual things or are they based on estimates,
projections, or outright guesses?
□ Have numbers concerning specific individuals and companies been verified using the original
records or checked against a sample if the data have more than 200 names?
□ Has a data team member written a methodology section covering data gathering and
analysis?
□ Has a different member of the data team reviewed the methodology other than the
methodology writer?
□ Are all maps, graphics, searchable databases, in house databases, and other data material
using the same exact data? That is to say, have changes been made to one database that
haven't been made to another?
□ Do all Web and other publishing components using the data have the necessary caveats,
warnings, data use suggestions, etc.?
□ Do the results look too good to be true (then they probably are)?
AJC newsroom’s favorite tip for bulletproofing stories:
Avoid surprises.
If a source, story subject, expert, institution, etc. is going to push back on your reporting or say it
is wrong, it is much better for that to happen before you publish. As much as possible/practical,
you should reveal what you plan to write and data you plan to publish before it is in final form.
•
•
•
•

Make time to share findings (as early as possible) with any investigative “targets.”
Consider reading back key portions of the story as you approach publication.
Consider a verification process to test all findings with all covered organizations if there
are many. (Mail merge..)
If you get new insight or knowledge in final fact-checking, be prepared to push back
publication. New insight could change your story more than you expect and mistakes
occur when making hasty, extensive changes.

