Making Data-Informed
Design Decisions
Tyler Fisher, NPR Visuals
Josh Kadis, Alley Interactive

Hello!
@tylrfishr && @kadisco

What Multivariate
Testing Looks Like
An Example from NPR Visuals

Designing a multivariate test
1. Determine an action to measure
2. Form a hypothesis
3. Test the hypothesis against a control
scenario

npr.org/lovestory

Variations On A Theme

Users don’t need share buttons.

What else could a user do
at the end of a story?

How can we encourage
users to take action?

Our Hypothesis
Users would be more likely to take action if we
presented them with a yes or no question that
asked them how the story made them feel.
We called this question the Care Question.

Our other, smaller hypothesis
Either asking users to follow us on social
media or support NPR would be more
successful than share buttons.

Our Test Model

Five possible paths
1. Follow Us
2. Support NPR
3. Care Question, User clicks Yes, Follow us
4. Care Question, User clicks Yes, Support NPR
5. Care Question, User clicks No, email us

NO QUESTION
FOLLOW

SUPPORT

CARE QUESTION

Measuring Results
● Some code to divide tests equally
● Track all clicks in Google Analytics
● Make sure you get the full report from GA,
not samples.

Determining Success

The Easy Math
1. Set a goal.
2. Decide how certain you need to be.
3. Calculate sample size.
4. Wait.
5. Seriously, wait.
6. We have a winner!

The Hard Math
1. Set a goal.
2. Decide how certain you need to be.
3. Start the test.
4. High performers pick up steam.
5. Low performers fall away.
6. Test ends as soon as there’s a winner.

Which math is right for you?
Easy
Rolling your own

Hard

X

GA Experiment/Optimizely

X

Minimizes test length

X

Keeps it simple
Handles many variations

X
X

Our Results
Test

Tests Run

Conversions

Conversion Rate

Confidence
Interval

Support Control

37554

69

0.184%

N/A

Support
Hypothesis

37795

723

1.913%

99.9%

What does it all mean?
● The Care Question worked!
● (We lost some data with bad Google
Analytics-fu)
● People will click a button that asks them to
donate to NPR.

What next?
Apply learnings, keep iterating on the test,
learn more.

Newsroom teams don’t have
products. We have stories.
Instead of iterating on product, iterate on
story form.

Testing stories
requires compromise

Traffic considerations

The next story will
always be different

npr.org/ebola

We knew the
Care Question worked.
The next test: Refining the question

Our Hypothesis
A Care Question tuned to the content of the
story would be the most successful question.

Our Results
Test

Tests Run

Conversions

Conversion Rate

Confidence Interval

Control

1344

27

2.01%

N/A

Did you like this
story?

1342

42

3.13%

96.69%

Did you like this
story? (It helps us to
know)

1364

55

4.03%

99.90%

Does this kind of
reporting matter to
you?

1358

97

7.14%

99.90%

Does this kind of
reporting matter to
you? (It helps us to
know)

1306

67

5.13%

99.90%

What does it all mean?
● The Care Question worked again!
● Tailoring the question to the content worked
better.

Product testing
considerations

Go deep.

Not all pageviews count.

Keep test groups
consistent.

An inconclusive test is
not a failed test.

Test product, not content

Trust your instincts.

Running tests on editorial
content: Is it ethical?

A counterexample

Have a line that
you won’t cross.

Why you want a culture of
testing on your team

You can know if
something worked

You can focus on iteration
and small improvements

You can define goals and
work to meet them

How to show more ads
I’m not kidding.

Served impressions vs.
viewed impressions

How viewability works

Why should journalists
care?

Make the case for your
content.

Actually kind of useful

Questions?
@tylrfishr && @kadisco

