```json
{
  "id": 5388,
  "authors": [],
  "conference": "ICLR",
  "year": "Not Listed",
  "title": "Learning Latent Representations of Words and Phrases With the Contextualized Word2Vec",
  "description": "This paper presents a new method for learning latent representations of words and phrases that combines the strengths of word2vec and contextualized language models. The method, called Contextualized Word2Vec, learns a two-level representation of words and phrases, where the first level is a dense vector representation of the word or phrase, and the second level is a context-dependent vector representation of the word or phrase. The method is evaluated on a variety of natural language processing tasks, including text classification, named entity recognition, and question answering, and is shown to outperform both word2vec and contextualized language models on all tasks.",
  "keywords": [
    "Natural Language Processing",
    "Word Embeddings",
    "Contextualized Language Models"
  ]
}
```
